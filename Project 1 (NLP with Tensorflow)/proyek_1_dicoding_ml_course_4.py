# -*- coding: utf-8 -*-
"""Proyek 1 - dicoding ML course 4

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19fswgysDPu9Ih0jII-OBcHpHVBSOZqZc

# **Proyek 1: Natural Language Processing**

### Nico Marcelino

## Downloading Data
"""

# install package and upload API key
!pip install -q kaggle
from google.colab import files
files.upload()

# change permissions and make directory
!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json
!ls ~/.kaggle

# download and unzip file
!kaggle datasets download -d hgultekin/bbcnewsarchive

!mkdir bbcnewsarchive.zip
!unzip bbcnewsarchive.zip -d bbcnewsarchive
!ls bbcnewsarchive

"""## Preparing Dataset"""

# make dataframe
import pandas as pd

df = pd.read_csv('bbcnewsarchive/bbc-news-data.csv', sep='\t')
df = df.drop(columns=['title', 'filename'])
df.head()

"""## Data Preprocessing"""

# one-hot encoding
category = pd.get_dummies(df.category)
new_df = pd.concat([df, category], axis=1)
new_df = new_df.drop(columns='category')
new_df

# import libraries
import nltk, re, string
from nltk.corpus import stopwords
from nltk.corpus import wordnet as wn
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('omw-1.4')

# make all characters lowercase
new_df['content'] = new_df['content'].str.lower()

# removing stopwords
stop_words = stopwords.words('english')
new_df['content'] = new_df['content'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))

# non-alphumeric character handling
new_df["content"] = new_df['content'].str.replace('[^\w\s]','')

new_df

# membuat set training dan test dari content dan category
from sklearn.model_selection import train_test_split

content = new_df['content'].values
category = new_df[['business', 'entertainment', 'politics', 'sport', 'tech']].values
content_train, content_test, category_train, category_test = train_test_split(content, category, test_size=0.2, shuffle=True)

# tokenisasi
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
     
tokenizer = Tokenizer(num_words=10000, oov_token='x')
tokenizer.fit_on_texts(content_train)

train_sequence = tokenizer.texts_to_sequences(content_train)
test_sequence = tokenizer.texts_to_sequences(content_test)

train_padded = pad_sequences(train_sequence, maxlen=50, padding='post', truncating='post') 
test_padded = pad_sequences(test_sequence, maxlen=50, padding='post', truncating='post')

# model building
import tensorflow as tf
from keras.layers import LSTM

model = tf.keras.Sequential([
        tf.keras.layers.Embedding(input_dim=10000, output_dim=32),
        tf.keras.layers.LSTM(128),
        tf.keras.layers.Dense(128),
        tf.keras.layers.Dropout(0.5),
        tf.keras.layers.Dense(5, activation='softmax')
])

model.compile(loss='categorical_crossentropy', optimizer='adam' ,metrics=['accuracy'])

model.summary()

# callback ketika kedua akurasi >90%
class myCallback(tf.keras.callbacks.Callback):
    def on_epoch_end(self, epoch, logs={}):
      if(logs.get('accuracy')>0.90 and logs.get('val_accuracy')>0.90):
        print("\nBoth accuracies are at over 90%!")
        self.model.stop_training = True
callbacks = myCallback()

# model fitting
num_epochs = 50

history = model.fit(train_padded, category_train, epochs=num_epochs, shuffle=True,
                    validation_data=(test_padded, category_test), verbose=2, callbacks=[callbacks])

# plot loss dari model
import matplotlib.pyplot as plt
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='lower left')
plt.show()

# plot akurasi dari model 
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='lower left')
plt.show()